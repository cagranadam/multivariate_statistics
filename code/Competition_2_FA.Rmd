---
title: "Competition II Day 3"
author: "Roma Siugzdaite"
date: "2025-05-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Competition II. Factor analysis

This dataset is about Student Habits vs Academic Performance

This is a simulated dataset exploring how lifestyle habits affect academic performance in students. With 1,000 synthetic student records and 15+ features including study hours, sleep patterns, social media usage, diet quality, mental health, and final exam scores, itâ€™s perfect for ML projects, regression analysis, clustering, and data viz. Created using realistic patterns for educational practice.

Ever wondered how much Netflix, sleep, or TikTok scrolling affects your grades? ðŸ‘€ This dataset simulates 1,000 students' daily habitsâ€”from study time to mental healthâ€”and compares them to final exam scores. It's like spying on your GPA through the lens of lifestyle. 

Data:
student_habits_performance.csv 

#Research question: How many groups are there?


```{r, echo=TRUE }

library(readr)
student_habits_performance <- read_csv("C:\\Users\\cgranadamunoz\\OneDrive - Universiteit Antwerpen\\Codes_UA\\multivariate_statistics\\student_habits_performance.csv")
head(student_habits_performance)
```

## Descriptive statistics

```{r, echo=TRUE }
# select only numerical variables
data <- student_habits_performance[, c(2, 4:6, 8:9, 11, 14, 16)]

# what function summarizes basic statistics? 

summary(data)
```

## Normality testing

```{r, echo=TRUE }

# for the next step of visualization it is good that your data are in a data frame
data <- as.data.frame(data)

#draw a histogram here:
for (col in names(data)) {
  hist(data[[col]], main = col)
}

# advanced step!!! 
# show all histograms in one figure
par(mfrow=c(3,3)) # 2x4 grid for variable visualization
for (i in seq(1,ncol(data))){
    hist(data[,i],main = '',xlab = colnames(data)[i])
}

```

## Shapiro test for normality

Shapiro Test
H0 = Normal Distribution
H1 = Not normal distribution
## is p > 0.05 is normal!!

We can create a table of all results and keep the test values.

```{r}

# run normality test

for (col in names(data)){
  cat("Variable:", col, "\n")
  print(shapiro.test(data[[col]]))
  }

## advanced step: put all variables in a table

normal_uni_test <- data.frame(matrix(0,ncol(data),3))
rownames(normal_uni_test) <- colnames(data)
colnames(normal_uni_test) <- c('Statistic','p-value','Normal')

alpha <- 0.05
for (i in seq(1,ncol(data))){
    normal_uni_test[i,1] <- shapiro.test(as.numeric(data[,i]))$statistic
    normal_uni_test[i,2] <- shapiro.test(as.numeric(data[,i]))$p.value
    if (normal_uni_test[i,2] < alpha){
        normal_uni_test[i,3] <- 'NO'
    } else{
        normal_uni_test[i,3] <- 'YES'
    }
}
normal_uni_test
View(normal_uni_test)
# if you want to save it in a file, do this
#write.csv(normal_uni_test, file = 'univariate_normal_test.csv')

```

## Correlation

Plot your correlation matrix

```{r}

# calculate and plot correlations
library(RcmdrMisc)
library(psych)
r <- rcorr.adjust(data, type = "pearson", use = "complete.obs")
corPlot(r$R$r, numbers = TRUE, stars = TRUE, diag = FALSE)


# New step:
# you can use also a new library, example here
library(ggplot2)
library(corrplot)
r <- cor(data, method = "spearman")
corrplot(r, method = 'square')

```
## Bartlet test

```{r}
library(psych)
#Very important to check

cortest.bartlett(data)

#Test from the Bartletâ€™s test shows that the correlation matrix is not an identity matrix since we have a p-value < 0.05. These results suggest that our data is well-suited for factor analysis.
```
Test from the Bartletâ€™s test shows that the correlation matrix is not an identity matrix since we have a p-value < 0.05. These results suggest that our data is well-suited for factor analysis.


```{r}

library(psych)
KMO(data)
#if is small is ok!!

```
Overall measure of sampling adequacy (MSA) is very small 0.17, so unacceptable. And individual MSA for variables is small as well, exept age. Meaning that there are problems with grouping variables in a strightforward way.

KMO has to be more than 0.6 for factor analysis. We have 0.17. :( Indicating that data are not clustered well inside the dataset.


## KMO (Kaiser-Meyer-Olkin factor adequacy) Test
Kaiser provided the following values for interpreting the results:

* 0.00 to 0.49 unacceptable
* 0.50 to 0.59 miserable
* 0.60 to 0.69 mediocre
* 0.70 to 0.79 middling
* 0.80 to 0.89 meritorious
* 0.90 to 1.00 marvelous


## Scree plot

```{r}

scree(data, pc=FALSE) 

```

```{r}
# Double check number if factors
fa.parallel(data, fa="fa")
```

How many factors you will take to the next step? 
R: Two factors are determined by the Scree plot analysis.
## Factor analysis

The way we learned in the class.
How many factors we will do? 
What is a cutoff for factors visibility? Here it is 0.3, you can choose 0.4 as you wish.


```{r}

Nfacs <- 3   # This is a number of factors you take for the Factor analysis. You can change this as needed.
# 6 factor are too many for 9 variables :(

library(stats)
fit <- factanal(data, Nfacs, rotation="varimax")

print(fit, digits=2, cutoff=0.3, sort=TRUE)

```

What Cumulative variance tell you? 
Is low cumulative variance, but the groups conceptually help to understand the data.

What the hypothesis about a number of sufficient factors tell you? 
The hypothesis show that p value is p<0.05 so 3 factors are not sufficient :(

Visualize factors using loadings of the variables

```{r}
# number of loadings is the number of factors
load <- fit$loadings[,1:Nfacs]
load
plot(load,type="p") # set up plot
text(load,labels=names(data),cex=.7)

```

Finally visualize the diagram of Factor analysis.

```{r}

library(psych)

loads <- fit$loadings

fa.diagram(loads)


```


```{r}
#graph
library(stats)
pca.cov = princomp(data,cor = TRUE)  # By default, it uses the covariance matrix to perform PCA
# if you want to use correlations add cor = TRUE

summary(pca.cov, loadings = T)
biplot(pca.cov)
```
```{r}
dim(fit$loadings)
#Change the values of the columns
round(fit$loadings[1:10,], 2)
```



## EXTRA

If I want to name your factors you can use another function. Check always what is your data name.

```{r}

factor_analysis <- fa(data,nfactors=3,fm='ml',rotate='varimax',max.iter=100)
factor_analysis

# Name your latent factors. if you have two factors - your have t give to names. If you have 3 factors, you have to give 3 names, and ect.
colnames(factor_analysis$loadings) <- c('Studying', 'Mental_health','Enviroment')
fa.diagram(factor_analysis,rsize = 0.5,cex=0.01)

```

## Your interpretation about the data? 
They statistics show that is not feasible categorize the data with 3 or 2 factors. The p values are low rejecting the hypothesis of 3 factors and the cumulative variance is low 0.39. So they dont describe well the variance of the data.


